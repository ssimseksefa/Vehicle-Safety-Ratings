{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the filtered and final crash files for each year\n",
    "\n",
    "crash_2016 = pd.read_csv('data/crash/crash_2016.csv')\n",
    "crash_2017 = pd.read_csv('data/crash/crash_2017.csv')\n",
    "crash_2018 = pd.read_csv('data/crash/crash_2018.csv')\n",
    "\n",
    "crash_list = [crash_2016, crash_2017, crash_2018]\n",
    "crash = pd.concat(crash_list)\n",
    "\n",
    "crash.loc[crash.trav_sp==99,'trav_sp'] = 999\n",
    "crash.loc[crash.trav_sp==98,'trav_sp'] = 999\n",
    "\n",
    "crash = crash.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200) # to display all 51 columns \n",
    "crash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(crash.mdlyr_im.value_counts()[crash.mdlyr_im.value_counts().index.isin(range(1995,2020))].sort_index().index, \n",
    "         crash.mdlyr_im.value_counts()[crash.mdlyr_im.value_counts().index.isin(range(1995,2020))].sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of 2008-2011 year vehicles are lower than expected. It must be related with financial crisis of 2007-2008 and its long-term effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damage to people / first level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantizing the severity of damage to people in a vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I need to create a column which will show the severity of damage from 100 (fatal) to 0 (no injury)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: Injured, Severity Unknown must be in between \"Minor Injury\" (50) and \"Serious Injury\" (75). Hence it is quantized as 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numbers of each injury severity levels\n",
    "crash.inj_sev_rate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by make_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_inj_sev = crash[['make_model', 'inj_sev_rate', 'vinyear']].groupby('make_model').agg(['mean', 'count'])\n",
    "avg_inj_sev.columns = ['avg_inj_sev', 'number_of_accidents', 'avg_mod_year', 'count']\n",
    "avg_inj_sev = avg_inj_sev.drop(columns='count')\n",
    "avg_inj_sev.avg_mod_year = round(avg_inj_sev.avg_mod_year, 1)\n",
    "avg_inj_sev = avg_inj_sev.sort_values('avg_inj_sev',ascending=True)\n",
    "avg_inj_sev = avg_inj_sev.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be the minimum sample number to be able to speak statistically a make/model is safe or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample number of specific make/model\n",
    "n = 50\n",
    "avg_inj_sev[avg_inj_sev.number_of_accidents>n].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maximum severity of injury shows only the severity of the most injured person in a vehicle.\n",
    "\n",
    "Another important factor is what percentage of people in vehicle got injured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating weighted severity of injury: inury severity * number of injured / number of occupants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_inj_sev_rate = inj_sev_rate*numinj/numoccs\n",
    "# This is to make weighting process smoother, if inj_sev=100, I do not want to decrease it to 0, it`ll decrease the number 1/2*inj_sev at most\n",
    "crash.weighted_inj_sev_rate = (crash.inj_sev_rate+crash.weighted_inj_sev_rate)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wgh_inj_sev = crash[['make_model', 'weighted_inj_sev_rate', 'vinyear']].groupby('make_model').agg(['mean', 'count'])\n",
    "avg_wgh_inj_sev.columns = ['avg_wgh_inj_sev', 'number_of_accidents', 'avg_mod_year', 'count']\n",
    "avg_wgh_inj_sev = avg_wgh_inj_sev.drop(columns='count')\n",
    "avg_wgh_inj_sev.avg_mod_year = round(avg_wgh_inj_sev.avg_mod_year, 1)\n",
    "avg_wgh_inj_sev = avg_wgh_inj_sev.sort_values('avg_wgh_inj_sev',ascending=True)\n",
    "avg_wgh_inj_sev = avg_wgh_inj_sev.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_wgh_inj_sev[avg_wgh_inj_sev.number_of_accidents>n].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to go deeper since it is obvious that not all the accident conditions are exact same. I should compare similiar conditions and results in order to see more accurate. Hence I need to create another level which will show the condition in which crash occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crash condition / second level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating new column to display and categorize travel speeds as multiples of 5 i.e. [0-5)-> 0, [5-10)-> 5, [10-15)-> 10, ... mph \n",
    "\n",
    "# speed_interval = pd.interval_range(start=0, freq=5, end=150, closed='left')\n",
    "# speed_list=[]\n",
    "# for i in range(0,150,5): speed_list.append(i)\n",
    "# crash['travel_speed'] = np.where(crash.trav_sp==997, \n",
    "#                                                        160,\n",
    "#                                                        pd.cut(crash.trav_sp, bins=speed_interval).apply(lambda x: x.left)\n",
    "#                                                       )\n",
    "# crash.travel_speed = crash.travel_speed.fillna(999).astype(int)\n",
    "\n",
    "# Doing the same work:\n",
    "\n",
    "crash['travel_speed'] = (crash.trav_sp/5).astype(int)*5\n",
    "\n",
    "crash.loc[(crash.travel_speed>=90) & (crash.travel_speed<100),'travel_speed'] = 90\n",
    "\n",
    "crash.loc[(crash.travel_speed>=100) & (crash.travel_speed<160),'travel_speed'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of vehicles older than 1989 is lower than 100, so I want to group them to see if there is a correlation between year and damage\n",
    "# Creating new column to display model years of old cars as group:\n",
    "# [:,1970)-> 1960, [1970,1980)-> 1970, [1980-1990)->1980\n",
    "\n",
    "crash['year'] = crash.mdlyr_im.astype(int)\n",
    "\n",
    "crash.loc[(crash.year<1990), 'year'] = (crash.mdlyr_im/10).astype(int)*10\n",
    "\n",
    "# crash['year'] = np.where(crash.mdlyr_im<1990,\n",
    "#                                                (crash.mdlyr_im/10).astype(int)*10,\n",
    "#                                                crash.mdlyr_im.astype(int) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The conditions that may have huge impact on the damage resulted. I want to see their effects and then isolate that effect if I could.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_list = ['travel_speed', \n",
    "                  'weathr_im', 'lgtcon_im',\n",
    "                  'mfactor', \n",
    "                  'vevent_im', 'impact1_im',\n",
    "                  'year',\n",
    "                  'vsurcond', \n",
    "                  'v_alch_im' ]\n",
    "\n",
    "crush_type_list = ['acc_type', 'rollover', 'deformed', 'fire_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a dataframe which contains all groups of all these categories. Hence I first created a function which group by by a category and returns the counts and avgerage severity of injury for each groups(which is code for each category/data elemnts). Then I merged these data for all conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the average of all conditions in 2016\n",
    "overall_avg = crash.weighted_inj_sev_rate.describe()[['mean', 'min', '25%', '50%', '75%', 'max']].mean()\n",
    "\n",
    "# Minimum number of crushes in order to be able to safely say that a condition is more dangerous or safe \n",
    "min_cnt = 25\n",
    "\n",
    "def category_avg(category, rank=0):\n",
    "    if rank==0:\n",
    "        damage='weighted_inj_sev_rate'\n",
    "    else:\n",
    "        damage='damage_over_condition'\n",
    "    overall_avg = crash[damage].describe()[['mean', 'min', '25%', '50%', '75%', 'max']].mean()\n",
    "    condition = crash[[category, damage]].groupby(category).describe()\n",
    "    condition.columns = [category+'_count', 'mean', 'std', 'min', '1q', '2q', '3q', 'max']\n",
    "    condition[category+'_avg_severity'] = condition[['mean', 'min', '1q', '2q', '3q', 'max']].mean(axis=1)\n",
    "    condition[category+'_relative_severity'] = (condition[category+'_avg_severity']-overall_avg).round(2)\n",
    "    condition = condition[condition[category+'_count']>min_cnt].reset_index()\n",
    "    condition = condition.drop(columns=[category+'_count',category+'_avg_severity', 'mean', 'std', 'min', '1q', '2q', '3q', 'max']).reset_index(drop=True)\n",
    "    \n",
    "    return condition\n",
    "\n",
    "conditions = pd.DataFrame()\n",
    "\n",
    "for i in range(len(condition_list+crush_type_list)):\n",
    "    \n",
    "    conditions = pd.merge(conditions, category_avg((condition_list+crush_type_list)[i]), left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "conditions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# category_avg('travel_speed',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'conditions' dataframe will show us the relative severity for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crash['crash_condition'] = 0\n",
    "\n",
    "crash['damage_over_condition'] = crash.weighted_inj_sev_rate - crash.crash_condition\n",
    "\n",
    "for i in range(len(condition_list)):\n",
    "    \n",
    "    if i==0:\n",
    "        damage='weighted_inj_sev_rate'\n",
    "    else:\n",
    "        damage='damage_over_condition'\n",
    "    \n",
    "    crash = pd.merge(crash, category_avg(condition_list[i],i), on= condition_list[i], how='left')\n",
    "        \n",
    "    crash[condition_list[i]+'_relative_severity'] = crash[condition_list[i]+'_relative_severity'].fillna(0)\n",
    "    \n",
    "    crash.loc[crash[condition_list[i]+'_relative_severity']<-10, condition_list[i]+'_relative_severity'] = -8 + (1/5)*crash[condition_list[i]+'_relative_severity']\n",
    "    crash.loc[((crash[condition_list[i]+'_relative_severity']>-10) & (crash[condition_list[i]+'_relative_severity']<-5)), condition_list[i]+'_relative_severity'] = -5 + (3/5)*crash[condition_list[i]+'_relative_severity']\n",
    "    crash.loc[((crash[condition_list[i]+'_relative_severity']<10) & (crash[condition_list[i]+'_relative_severity']>5)), condition_list[i]+'_relative_severity'] = 5 + (3/5)*crash[condition_list[i]+'_relative_severity']\n",
    "    crash.loc[crash[condition_list[i]+'_relative_severity']>10, condition_list[i]+'_relative_severity'] = 8 + (1/5)*crash[condition_list[i]+'_relative_severity']\n",
    "    \n",
    "    crash.crash_condition = crash.crash_condition + crash[condition_list[i]+'_relative_severity']\n",
    "    \n",
    "    crash.loc[(((crash[damage]<=0) & (crash[condition_list[i]+'_relative_severity'].fillna(0)>0)) | (crash[damage]!=0)),'damage_over_condition'] = crash.damage_over_condition - 1.5*crash[condition_list[i]+'_relative_severity'].fillna(0)\n",
    "        \n",
    "    crash = crash.drop(columns=condition_list[i]+'_relative_severity')\n",
    "\n",
    "crash.damage_over_condition = 100*(crash.damage_over_condition-crash.damage_over_condition.min())/(crash.damage_over_condition.max()-crash.damage_over_condition.min())\n",
    "\n",
    "crash['safety_rate'] = 100-crash.damage_over_condition\n",
    "\n",
    "crash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# z = np.abs(stats.zscore(crash.safety_rate))\n",
    "\n",
    "# crash[z>2].safety_rate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_safety_rate = crash[['make_model', 'safety_rate', 'vinyear', 'weighted_inj_sev_rate']].groupby('make_model').agg(['mean', 'count'])\n",
    "avg_safety_rate.columns = ['avg_safety_rate', 'number_of_accidents', 'avg_mod_year', 'count', 'weighted_inj_sev_rate', 'cnt']\n",
    "avg_safety_rate = avg_safety_rate.drop(columns=['count', 'cnt'])\n",
    "avg_safety_rate.avg_mod_year = round(avg_safety_rate.avg_mod_year, 1)\n",
    "avg_safety_rate = avg_safety_rate.sort_values('avg_safety_rate',ascending=True)\n",
    "avg_safety_rate = avg_safety_rate.reset_index()\n",
    "avg_safety_rate[avg_safety_rate.number_of_accidents>2].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It is time to get 2017 and 2018 crash data and merge into a final crash file. I will use the same metrics created in this notebook for combined data for three years(2016, 2017, 2018). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash = crash[~crash.make.isin(['DAEWOO', 'EAGLE', 'DATSUN', 'KENWORTH','FRUEHAUF','KAR-TOTE','THE VEHICLE PRODUCTION GROUP','FREIGHTLINER','WORKHORSE','AM GENERAL','CLASSIC MOTORCYCLES & SIDECARS','AMERICAN MOTORS','WORKHORSE CUSTOM CHASSIS','WILSON TRAILER CO'])].reset_index(drop=True)\n",
    "\n",
    "crash = crash[~crash.bdytyp_im.isin([11,41,12])].reset_index(drop=True)\n",
    "\n",
    "crash.loc[crash.make=='ROLLS-ROYCE','make'] = 'ROLLS ROYCE'\n",
    "\n",
    "crash.loc[crash.make=='RAM','model'] = 'RAM ' + crash.loc[crash.make=='RAM','model'].astype(str)\n",
    "\n",
    "crash.loc[crash.make=='RAM','make'] = 'DODGE'\n",
    "\n",
    "crash.make_model = crash.make + ' ' + crash.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_codes = [4, 14, 34, 6, 15, 2, 20, 31, 5, 3, 30, 1, 16, 9, 40, 8, 17, 39, 10, 19, 48, 12, 45, 7, 32, 42]\n",
    "\n",
    "body_names = ['Sedan', 'SUV', 'Pickup', 'Station Wagon', 'SUV', 'Sedan 2-Door/       Coupe',\n",
    "             'Minivan', 'Pickup', 'Hatchback', 'Hatchback/  2-Door',\n",
    "              'Pickup', 'Convertible', 'Station Wagon', 'Sedan', \n",
    "             'Pickup', 'Sedan', 'Sedan 2-Door/       Coupe', 'Pickup', 'Pickup', 'SUV', 'Pickup',\n",
    "             'Limousine', 'Pickup', 'Hatchback', 'Pickup','Pickup']\n",
    "\n",
    "body_decoding = pd.DataFrame(index=body_codes, columns=['body_type'],data=body_names)\n",
    "body_decoding = body_decoding.reset_index().rename(columns={'index':'bdytyp_im'})\n",
    "\n",
    "crash = pd.merge(crash,body_decoding, on='bdytyp_im', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding car price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_csv('data/msrp_data.csv')\n",
    "price.columns = price.columns.str.lower()\n",
    "price.make = price.make.str.upper()\n",
    "price.model = price.model.str.upper()\n",
    "price.loc[price.make=='ROLLS-ROYCE','make'] = 'ROLLS ROYCE'\n",
    "price['make_model'] = price.make + ' ' + price.model\n",
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_grouped = price[['make_model', 'year', 'msrp', 'highway mpg', 'city mpg']][(price.make_model.isin(crash.make_model)) & (price.year>2000)].groupby(by=['make_model', 'year']).mean().astype(int).reset_index()\n",
    "price_grouped = price_grouped.groupby('make_model').last().reset_index().rename(columns={'year':'msrp_year'})\n",
    "price_grouped.columns = ['make_model', 'msrp_year', 'msrp', 'highway_mpg', 'city_mpg']\n",
    "price_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crash = pd.merge(crash, price_grouped, on='make_model',how='left')\n",
    "crash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The ' + str(round(100*(crash.shape[0]-crash.msrp.isna().sum())/crash.shape[0],2)) + ' percent of the vehicles in my crash data are matched with the price data in terms of make-model')\n",
    "\n",
    "print('The ' + str(round(100*crash[~crash.msrp.isna()].make_model.unique().shape[0]/crash.make_model.unique().shape[0],2)) + ' percent of the Make-Model pairs in my crash data are matched with the price data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit = -3\n",
    "def zero_inj_rate(limit):\n",
    "    return 100*crash[((crash.crash_condition<=limit)&(crash.inj_sev_rate==0))].shape[0]/crash[((crash.crash_condition<=limit))].shape[0]\n",
    "zero_rate = []\n",
    "limit_range = []\n",
    "for i in range(-10,10):\n",
    "    zero_rate.append(zero_inj_rate(i))\n",
    "    limit_range.append(i)\n",
    "plt.plot(limit_range,zero_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the crash condition severity is well below the average and there is no injury, it does not show that the vehicle is safe! So I want to exlude that case.\n",
    "crash3 = crash[~((crash.crash_condition<=-3)&(crash.inj_sev_rate==0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash.to_csv('data/crash2.csv', index=False)\n",
    "crash3.to_csv('data/crash3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
